{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a460ee6c-ebee-495e-a582-b9b9cfeab405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1  Lasso Regression is a popular machine learning technique used for feature selection and regularization. It is a type of linear regression that adds a penalty term to the cost function of the model to prevent overfitting. Regularization is a common approach that can help make the model more generalized and improve its performance. Lasso Regression uses the L1 regularization technique, where the coefficients determined in the linear model are shrunk towards the central point as the mean by introducing a penalization factor called the alpha (or sometimes lamda) values.It is a popular method for reducing the complexity of a model while still maintaining its predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b147eaae-5ba9-41be-9dab-47a036ad460f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Automatic features selection. The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2ea5f06-93db-44b6-bd64-9ee050b0a8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 LaSSO (a penalized estimation method) aims at estimating the same quantities (model coefficients) as, say, OLS maximum likelihood (an unpenalized method). The model is the same, and the interpretation remains the same. The numerical values from LASSO will normally differ from those from OLS maximum likelihood: some will be closer to zero, others will be exactly zero. If a sensible amount of penalization has been applied, the LASSO estimates will lie closer to the true values than the OLS maximum likelihood estimates, which is a desirable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c78ecc9d-d504-45e3-8878-9c354f40d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4  A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean. Shrinkage results in simple, sparse models which are easier to analyze than high-dimensional data models with large numbers of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea3f3f8e-a64d-4986-9138-5532ebabd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5ou can fit a lasso regressor to the whole lot, multiplying out your brackets giving you 2m+2 coefficients. Then by performing a change of variables you can make this a linear regression problem again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72680bb3-6ddb-41f6-b192-ca3adadba103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 Ridge regression\n",
    "#Performs L2 regularization, i.e., adds penalty equivalent to the square of the magnitude of coefficients\n",
    "#Minimization objective = LS Obj + α * (sum of square of coefficients)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a0601e7-0cbc-417c-b48b-8fcd1a7bc1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lasso Regression\n",
    "#Performs L1 regularization, i.e., adds penalty equivalent to the absolute value of the magnitude of coefficients\n",
    "#Minimization objective = LS Obj + α * (sum of the absolute value of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b175a24-5a8b-49c8-9635-e6b03fa0f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 The advantage of this is clear when we have LOTS of PARAMETERS in the model:\n",
    "#In Ridge, when we increase the value of LAMBDA, the most important parameters might shrink a little bit and the less important parameter stay at high value. In contrast, with LASSO when we increase the value of LAMBDA the most important parameters shrink a little bit and the less important parameters goes closed to ZERO.\n",
    "#So, LASSO is able to exclude silly parameters from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd3212a-69e8-4710-bfed-943f163d2632",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
